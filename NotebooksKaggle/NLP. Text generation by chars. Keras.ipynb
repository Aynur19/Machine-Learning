{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-19T18:27:47.940209Z","iopub.execute_input":"2021-06-19T18:27:47.940581Z","iopub.status.idle":"2021-06-19T18:27:47.953255Z","shell.execute_reply.started":"2021-06-19T18:27:47.940546Z","shell.execute_reply":"2021-06-19T18:27:47.951764Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stdout","text":"/kaggle/lib/kaggle/gcp.py\n/kaggle/input/dataset-text/frankenstein_2.txt\n/kaggle/input/models-lstm/model_weights_saved_e10.hdf5\n/kaggle/working/__notebook_source__.ipynb\n/kaggle/output/kaggle/working/model_weights_saved_e20.hdf5\n/kaggle/output/kaggle/working/model_weights_saved_e200.hdf5\n/kaggle/output/kaggle/working/model_weights_saved_e100.hdf5\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy\nimport nltk\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import stopwords\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, LSTM\nfrom keras.utils import np_utils\nfrom keras.callbacks import ModelCheckpoint","metadata":{"execution":{"iopub.status.busy":"2021-06-19T18:27:48.465972Z","iopub.execute_input":"2021-06-19T18:27:48.466324Z","iopub.status.idle":"2021-06-19T18:27:48.471131Z","shell.execute_reply.started":"2021-06-19T18:27:48.466291Z","shell.execute_reply":"2021-06-19T18:27:48.470145Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"file = open('../input/dataset-text/frankenstein_2.txt').read()","metadata":{"execution":{"iopub.status.busy":"2021-06-19T18:27:49.889624Z","iopub.execute_input":"2021-06-19T18:27:49.889943Z","iopub.status.idle":"2021-06-19T18:27:49.895301Z","shell.execute_reply.started":"2021-06-19T18:27:49.889913Z","shell.execute_reply":"2021-06-19T18:27:49.894286Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"def tokenize_words(input):\n    # lowercase everything to standardize it\n    input = input.lower()\n\n    # instantiate the tokenizer\n    tokenizer = RegexpTokenizer(r'\\w+')\n    tokens = tokenizer.tokenize(input)\n\n    # if the created token isn't in the stop words, make it part of \"filtered\"\n    filtered = filter(lambda token: token not in stopwords.words('english'), tokens)\n    return \" \".join(filtered)","metadata":{"execution":{"iopub.status.busy":"2021-06-19T18:27:53.519293Z","iopub.execute_input":"2021-06-19T18:27:53.519633Z","iopub.status.idle":"2021-06-19T18:27:53.528165Z","shell.execute_reply.started":"2021-06-19T18:27:53.519600Z","shell.execute_reply":"2021-06-19T18:27:53.527166Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"nltk.download('stopwords')\n\n# preprocess the input data, make tokens\nprocessed_inputs = tokenize_words(file)","metadata":{"execution":{"iopub.status.busy":"2021-06-19T18:27:53.877074Z","iopub.execute_input":"2021-06-19T18:27:53.877432Z","iopub.status.idle":"2021-06-19T18:27:56.415184Z","shell.execute_reply.started":"2021-06-19T18:27:53.877400Z","shell.execute_reply":"2021-06-19T18:27:56.414290Z"},"trusted":true},"execution_count":59,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"chars = sorted(list(set(processed_inputs)))\nchar_to_num = dict((c, i) for i, c in enumerate(chars))","metadata":{"execution":{"iopub.status.busy":"2021-06-19T18:27:56.416732Z","iopub.execute_input":"2021-06-19T18:27:56.417214Z","iopub.status.idle":"2021-06-19T18:27:56.422743Z","shell.execute_reply.started":"2021-06-19T18:27:56.417176Z","shell.execute_reply":"2021-06-19T18:27:56.421762Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"input_len = len(processed_inputs)\nvocab_len = len(chars)\nprint (\"Total number of characters:\", input_len)\nprint (\"Total vocab:\", vocab_len)","metadata":{"execution":{"iopub.status.busy":"2021-06-19T18:27:56.424592Z","iopub.execute_input":"2021-06-19T18:27:56.425084Z","iopub.status.idle":"2021-06-19T18:27:56.437694Z","shell.execute_reply.started":"2021-06-19T18:27:56.425046Z","shell.execute_reply":"2021-06-19T18:27:56.436577Z"},"trusted":true},"execution_count":61,"outputs":[{"name":"stdout","text":"Total number of characters: 80535\nTotal vocab: 41\n","output_type":"stream"}]},{"cell_type":"code","source":"seq_length = 100\nx_data = []\ny_data = []","metadata":{"execution":{"iopub.status.busy":"2021-06-19T18:27:56.439370Z","iopub.execute_input":"2021-06-19T18:27:56.439800Z","iopub.status.idle":"2021-06-19T18:27:56.464905Z","shell.execute_reply.started":"2021-06-19T18:27:56.439762Z","shell.execute_reply":"2021-06-19T18:27:56.463726Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"# loop through inputs, start at the beginning and go until we hit\n# the final character we can create a sequence out of\nfor i in range(0, input_len - seq_length, 1):\n    # Define input and output sequences\n    # Input is the current character plus desired sequence length\n    in_seq = processed_inputs[i:i + seq_length]\n\n    # Out sequence is the initial character plus total sequence length\n    out_seq = processed_inputs[i + seq_length]\n\n    # We now convert list of characters to integers based on\n    # previously and add the values to our lists\n    x_data.append([char_to_num[char] for char in in_seq])\n    y_data.append(char_to_num[out_seq])","metadata":{"execution":{"iopub.status.busy":"2021-06-19T18:27:56.466493Z","iopub.execute_input":"2021-06-19T18:27:56.466858Z","iopub.status.idle":"2021-06-19T18:27:57.520752Z","shell.execute_reply.started":"2021-06-19T18:27:56.466818Z","shell.execute_reply":"2021-06-19T18:27:57.519859Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"n_patterns = len(x_data)\nprint (\"Total Patterns:\", n_patterns)","metadata":{"execution":{"iopub.status.busy":"2021-06-19T18:28:00.171398Z","iopub.execute_input":"2021-06-19T18:28:00.171763Z","iopub.status.idle":"2021-06-19T18:28:00.177380Z","shell.execute_reply.started":"2021-06-19T18:28:00.171730Z","shell.execute_reply":"2021-06-19T18:28:00.176353Z"},"trusted":true},"execution_count":64,"outputs":[{"name":"stdout","text":"Total Patterns: 80435\n","output_type":"stream"}]},{"cell_type":"code","source":"X = numpy.reshape(x_data, (n_patterns, seq_length, 1))\nX = X/float(vocab_len)","metadata":{"execution":{"iopub.status.busy":"2021-06-19T18:28:00.438205Z","iopub.execute_input":"2021-06-19T18:28:00.438567Z","iopub.status.idle":"2021-06-19T18:28:01.836940Z","shell.execute_reply.started":"2021-06-19T18:28:00.438534Z","shell.execute_reply":"2021-06-19T18:28:01.836054Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"y = np_utils.to_categorical(y_data)","metadata":{"execution":{"iopub.status.busy":"2021-06-19T18:28:01.838431Z","iopub.execute_input":"2021-06-19T18:28:01.838822Z","iopub.status.idle":"2021-06-19T18:28:01.860818Z","shell.execute_reply.started":"2021-06-19T18:28:01.838776Z","shell.execute_reply":"2021-06-19T18:28:01.859643Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"model = Sequential()\nmodel.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(256, return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(128))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(y.shape[1], activation='softmax'))","metadata":{"execution":{"iopub.status.busy":"2021-06-19T18:30:05.593765Z","iopub.execute_input":"2021-06-19T18:30:05.594123Z","iopub.status.idle":"2021-06-19T18:30:06.496205Z","shell.execute_reply.started":"2021-06-19T18:30:05.594088Z","shell.execute_reply":"2021-06-19T18:30:06.495285Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"filepath = \"../output/kaggle/working/model_weights_saved_e200.hdf5\"\nmodel.load_weights(filepath)\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')","metadata":{"execution":{"iopub.status.busy":"2021-06-19T18:30:06.497802Z","iopub.execute_input":"2021-06-19T18:30:06.498192Z","iopub.status.idle":"2021-06-19T18:30:06.530036Z","shell.execute_reply.started":"2021-06-19T18:30:06.498149Z","shell.execute_reply":"2021-06-19T18:30:06.529247Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"filepath = \"../output/kaggle/working/model_weights_saved_e300.hdf5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\ndesired_callbacks = [checkpoint]","metadata":{"execution":{"iopub.status.busy":"2021-06-19T18:30:06.804437Z","iopub.execute_input":"2021-06-19T18:30:06.804756Z","iopub.status.idle":"2021-06-19T18:30:06.811880Z","shell.execute_reply.started":"2021-06-19T18:30:06.804726Z","shell.execute_reply":"2021-06-19T18:30:06.810813Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"model.fit(X, y, epochs=100, batch_size=256, callbacks=desired_callbacks)","metadata":{"execution":{"iopub.status.busy":"2021-06-19T18:30:08.663816Z","iopub.execute_input":"2021-06-19T18:30:08.664172Z","iopub.status.idle":"2021-06-19T18:58:25.245919Z","shell.execute_reply.started":"2021-06-19T18:30:08.664127Z","shell.execute_reply":"2021-06-19T18:58:25.244946Z"},"trusted":true},"execution_count":76,"outputs":[{"name":"stdout","text":"Epoch 1/100\n315/315 [==============================] - 21s 54ms/step - loss: 1.3812\n\nEpoch 00001: loss improved from inf to 1.37959, saving model to ../output/kaggle/working/model_weights_saved_e300.hdf5\nEpoch 2/100\n315/315 [==============================] - 17s 53ms/step - loss: 1.3675\n\nEpoch 00002: loss did not improve from 1.37959\nEpoch 3/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.3660\n\nEpoch 00003: loss improved from 1.37959 to 1.37698, saving model to ../output/kaggle/working/model_weights_saved_e300.hdf5\nEpoch 4/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.3661\n\nEpoch 00004: loss improved from 1.37698 to 1.37431, saving model to ../output/kaggle/working/model_weights_saved_e300.hdf5\nEpoch 5/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.3728\n\nEpoch 00005: loss improved from 1.37431 to 1.37287, saving model to ../output/kaggle/working/model_weights_saved_e300.hdf5\nEpoch 6/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.3731\n\nEpoch 00006: loss improved from 1.37287 to 1.37286, saving model to ../output/kaggle/working/model_weights_saved_e300.hdf5\nEpoch 7/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.3642\n\nEpoch 00007: loss improved from 1.37286 to 1.37082, saving model to ../output/kaggle/working/model_weights_saved_e300.hdf5\nEpoch 8/100\n315/315 [==============================] - 17s 53ms/step - loss: 1.3687\n\nEpoch 00008: loss improved from 1.37082 to 1.36792, saving model to ../output/kaggle/working/model_weights_saved_e300.hdf5\nEpoch 9/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.3544\n\nEpoch 00009: loss did not improve from 1.36792\nEpoch 10/100\n315/315 [==============================] - 17s 53ms/step - loss: 1.3645\n\nEpoch 00010: loss improved from 1.36792 to 1.36727, saving model to ../output/kaggle/working/model_weights_saved_e300.hdf5\nEpoch 11/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.3642\n\nEpoch 00011: loss improved from 1.36727 to 1.36480, saving model to ../output/kaggle/working/model_weights_saved_e300.hdf5\nEpoch 12/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.3571\n\nEpoch 00012: loss improved from 1.36480 to 1.36398, saving model to ../output/kaggle/working/model_weights_saved_e300.hdf5\nEpoch 13/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.3552\n\nEpoch 00013: loss improved from 1.36398 to 1.36275, saving model to ../output/kaggle/working/model_weights_saved_e300.hdf5\nEpoch 14/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.3675\n\nEpoch 00014: loss did not improve from 1.36275\nEpoch 15/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.3595\n\nEpoch 00015: loss did not improve from 1.36275\nEpoch 16/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.3602\n\nEpoch 00016: loss did not improve from 1.36275\nEpoch 17/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.3621\n\nEpoch 00017: loss did not improve from 1.36275\nEpoch 18/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.3577\n\nEpoch 00018: loss improved from 1.36275 to 1.36184, saving model to ../output/kaggle/working/model_weights_saved_e300.hdf5\nEpoch 19/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.3547\n\nEpoch 00019: loss improved from 1.36184 to 1.35903, saving model to ../output/kaggle/working/model_weights_saved_e300.hdf5\nEpoch 20/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.3515\n\nEpoch 00020: loss improved from 1.35903 to 1.35694, saving model to ../output/kaggle/working/model_weights_saved_e300.hdf5\nEpoch 21/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.3458\n\nEpoch 00021: loss improved from 1.35694 to 1.35349, saving model to ../output/kaggle/working/model_weights_saved_e300.hdf5\nEpoch 22/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.3526\n\nEpoch 00022: loss did not improve from 1.35349\nEpoch 23/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.3518\n\nEpoch 00023: loss did not improve from 1.35349\nEpoch 24/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.3428\n\nEpoch 00024: loss improved from 1.35349 to 1.35126, saving model to ../output/kaggle/working/model_weights_saved_e300.hdf5\nEpoch 25/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.3565\n\nEpoch 00025: loss improved from 1.35126 to 1.34968, saving model to ../output/kaggle/working/model_weights_saved_e300.hdf5\nEpoch 26/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.3429\n\nEpoch 00026: loss did not improve from 1.34968\nEpoch 27/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.3478\n\nEpoch 00027: loss did not improve from 1.34968\nEpoch 28/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.3400\n\nEpoch 00028: loss improved from 1.34968 to 1.34636, saving model to ../output/kaggle/working/model_weights_saved_e300.hdf5\nEpoch 29/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.3461\n\nEpoch 00029: loss did not improve from 1.34636\nEpoch 30/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.3505\n\nEpoch 00030: loss did not improve from 1.34636\nEpoch 31/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.3431\n\nEpoch 00031: loss improved from 1.34636 to 1.34125, saving model to ../output/kaggle/working/model_weights_saved_e300.hdf5\nEpoch 32/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.3371\n\nEpoch 00032: loss did not improve from 1.34125\nEpoch 33/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.3440\n\nEpoch 00033: loss did not improve from 1.34125\nEpoch 34/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.3440\n\nEpoch 00034: loss did not improve from 1.34125\nEpoch 35/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.3290\n\nEpoch 00035: loss improved from 1.34125 to 1.33942, saving model to ../output/kaggle/working/model_weights_saved_e300.hdf5\nEpoch 36/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.3364\n\nEpoch 00036: loss did not improve from 1.33942\nEpoch 37/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.3383\n\nEpoch 00037: loss did not improve from 1.33942\nEpoch 38/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.3355\n\nEpoch 00038: loss did not improve from 1.33942\nEpoch 39/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.3363\n\nEpoch 00039: loss did not improve from 1.33942\nEpoch 40/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.3301\n\nEpoch 00040: loss improved from 1.33942 to 1.33539, saving model to ../output/kaggle/working/model_weights_saved_e300.hdf5\nEpoch 41/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.3248\n\nEpoch 00041: loss improved from 1.33539 to 1.33323, saving model to ../output/kaggle/working/model_weights_saved_e300.hdf5\nEpoch 42/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.3309\n\nEpoch 00042: loss did not improve from 1.33323\nEpoch 43/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.3237\n\nEpoch 00043: loss improved from 1.33323 to 1.33132, saving model to ../output/kaggle/working/model_weights_saved_e300.hdf5\nEpoch 44/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.3302\n\nEpoch 00044: loss did not improve from 1.33132\nEpoch 45/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.3307\n\nEpoch 00045: loss did not improve from 1.33132\nEpoch 46/100\n315/315 [==============================] - 17s 53ms/step - loss: 1.3169\n\nEpoch 00046: loss improved from 1.33132 to 1.32196, saving model to ../output/kaggle/working/model_weights_saved_e300.hdf5\nEpoch 47/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.3242\n\nEpoch 00047: loss did not improve from 1.32196\nEpoch 48/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.3189\n\nEpoch 00048: loss did not improve from 1.32196\nEpoch 49/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.3222\n\nEpoch 00049: loss did not improve from 1.32196\nEpoch 50/100\n315/315 [==============================] - 17s 53ms/step - loss: 1.3159\n\nEpoch 00050: loss did not improve from 1.32196\nEpoch 51/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.3123\n\nEpoch 00051: loss did not improve from 1.32196\nEpoch 52/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.3166\n\nEpoch 00052: loss improved from 1.32196 to 1.32063, saving model to ../output/kaggle/working/model_weights_saved_e300.hdf5\nEpoch 53/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.3291\n\nEpoch 00053: loss did not improve from 1.32063\nEpoch 54/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.3222\n\nEpoch 00054: loss did not improve from 1.32063\nEpoch 55/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.3168\n\nEpoch 00055: loss did not improve from 1.32063\nEpoch 56/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.3135\n\nEpoch 00056: loss improved from 1.32063 to 1.31893, saving model to ../output/kaggle/working/model_weights_saved_e300.hdf5\nEpoch 57/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.3239\n\nEpoch 00057: loss improved from 1.31893 to 1.31745, saving model to ../output/kaggle/working/model_weights_saved_e300.hdf5\nEpoch 58/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.3038\n\nEpoch 00058: loss improved from 1.31745 to 1.31074, saving model to ../output/kaggle/working/model_weights_saved_e300.hdf5\nEpoch 59/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.3133\n\nEpoch 00059: loss did not improve from 1.31074\nEpoch 60/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.3018\n\nEpoch 00060: loss did not improve from 1.31074\nEpoch 61/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.3140\n\nEpoch 00061: loss did not improve from 1.31074\nEpoch 62/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.3011\n\nEpoch 00062: loss did not improve from 1.31074\nEpoch 63/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.3128\n\nEpoch 00063: loss did not improve from 1.31074\nEpoch 64/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.3100\n\nEpoch 00064: loss improved from 1.31074 to 1.30762, saving model to ../output/kaggle/working/model_weights_saved_e300.hdf5\nEpoch 65/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.3034\n\nEpoch 00065: loss did not improve from 1.30762\nEpoch 66/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.3116\n\nEpoch 00066: loss did not improve from 1.30762\nEpoch 67/100\n315/315 [==============================] - 17s 53ms/step - loss: 1.3066\n\nEpoch 00067: loss did not improve from 1.30762\nEpoch 68/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.2942\n\nEpoch 00068: loss improved from 1.30762 to 1.30759, saving model to ../output/kaggle/working/model_weights_saved_e300.hdf5\nEpoch 69/100\n315/315 [==============================] - 17s 53ms/step - loss: 1.3059\n\nEpoch 00069: loss did not improve from 1.30759\nEpoch 70/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.3085\n\nEpoch 00070: loss did not improve from 1.30759\nEpoch 71/100\n315/315 [==============================] - 17s 53ms/step - loss: 1.3097\n\nEpoch 00071: loss did not improve from 1.30759\nEpoch 72/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.3038\n\nEpoch 00072: loss improved from 1.30759 to 1.30613, saving model to ../output/kaggle/working/model_weights_saved_e300.hdf5\nEpoch 73/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.2894\n\nEpoch 00073: loss improved from 1.30613 to 1.30252, saving model to ../output/kaggle/working/model_weights_saved_e300.hdf5\nEpoch 74/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.3063\n\nEpoch 00074: loss improved from 1.30252 to 1.30193, saving model to ../output/kaggle/working/model_weights_saved_e300.hdf5\nEpoch 75/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.2922\n\nEpoch 00075: loss did not improve from 1.30193\nEpoch 76/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.3019\n\nEpoch 00076: loss did not improve from 1.30193\nEpoch 77/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.2961\n\nEpoch 00077: loss did not improve from 1.30193\nEpoch 78/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.2961\n\nEpoch 00078: loss improved from 1.30193 to 1.29972, saving model to ../output/kaggle/working/model_weights_saved_e300.hdf5\nEpoch 79/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.2927\n\nEpoch 00079: loss improved from 1.29972 to 1.29832, saving model to ../output/kaggle/working/model_weights_saved_e300.hdf5\nEpoch 80/100\n315/315 [==============================] - 17s 53ms/step - loss: 1.3017\n\nEpoch 00080: loss did not improve from 1.29832\nEpoch 81/100\n315/315 [==============================] - 17s 53ms/step - loss: 1.2999\n\nEpoch 00081: loss did not improve from 1.29832\nEpoch 82/100\n315/315 [==============================] - 17s 53ms/step - loss: 1.2868\n\nEpoch 00082: loss improved from 1.29832 to 1.29389, saving model to ../output/kaggle/working/model_weights_saved_e300.hdf5\nEpoch 83/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.2841\n\nEpoch 00083: loss did not improve from 1.29389\nEpoch 84/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.2880\n\nEpoch 00084: loss improved from 1.29389 to 1.29282, saving model to ../output/kaggle/working/model_weights_saved_e300.hdf5\nEpoch 85/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.2893\n\nEpoch 00085: loss did not improve from 1.29282\nEpoch 86/100\n315/315 [==============================] - 17s 53ms/step - loss: 1.2886\n\nEpoch 00086: loss did not improve from 1.29282\nEpoch 87/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.2879\n\nEpoch 00087: loss did not improve from 1.29282\nEpoch 88/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.2809\n\nEpoch 00088: loss improved from 1.29282 to 1.28862, saving model to ../output/kaggle/working/model_weights_saved_e300.hdf5\nEpoch 89/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.2929\n\nEpoch 00089: loss did not improve from 1.28862\nEpoch 90/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.2794\n\nEpoch 00090: loss did not improve from 1.28862\nEpoch 91/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.2919\n\nEpoch 00091: loss improved from 1.28862 to 1.28772, saving model to ../output/kaggle/working/model_weights_saved_e300.hdf5\nEpoch 92/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.3007\n\nEpoch 00092: loss did not improve from 1.28772\nEpoch 93/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.2716\n\nEpoch 00093: loss improved from 1.28772 to 1.28443, saving model to ../output/kaggle/working/model_weights_saved_e300.hdf5\nEpoch 94/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.2787\n\nEpoch 00094: loss did not improve from 1.28443\nEpoch 95/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.2801\n\nEpoch 00095: loss did not improve from 1.28443\nEpoch 96/100\n315/315 [==============================] - 17s 53ms/step - loss: 1.2776\n\nEpoch 00096: loss improved from 1.28443 to 1.28389, saving model to ../output/kaggle/working/model_weights_saved_e300.hdf5\nEpoch 97/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.2723\n\nEpoch 00097: loss improved from 1.28389 to 1.28120, saving model to ../output/kaggle/working/model_weights_saved_e300.hdf5\nEpoch 98/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.2761\n\nEpoch 00098: loss improved from 1.28120 to 1.28079, saving model to ../output/kaggle/working/model_weights_saved_e300.hdf5\nEpoch 99/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.2760\n\nEpoch 00099: loss improved from 1.28079 to 1.27812, saving model to ../output/kaggle/working/model_weights_saved_e300.hdf5\nEpoch 100/100\n315/315 [==============================] - 17s 54ms/step - loss: 1.2789\n\nEpoch 00100: loss did not improve from 1.27812\n","output_type":"stream"},{"execution_count":76,"output_type":"execute_result","data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7f077a1ab950>"},"metadata":{}}]},{"cell_type":"code","source":"filename = \"../output/kaggle/working/model_weights_saved_e300.hdf5\"\nmodel.load_weights(filename)\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')","metadata":{"execution":{"iopub.status.busy":"2021-06-19T18:58:55.719747Z","iopub.execute_input":"2021-06-19T18:58:55.720075Z","iopub.status.idle":"2021-06-19T18:58:55.747335Z","shell.execute_reply.started":"2021-06-19T18:58:55.720044Z","shell.execute_reply":"2021-06-19T18:58:55.746459Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"num_to_char = dict((i, c) for i, c in enumerate(chars))","metadata":{"execution":{"iopub.status.busy":"2021-06-19T18:58:58.370603Z","iopub.execute_input":"2021-06-19T18:58:58.370945Z","iopub.status.idle":"2021-06-19T18:58:58.374833Z","shell.execute_reply.started":"2021-06-19T18:58:58.370911Z","shell.execute_reply":"2021-06-19T18:58:58.373831Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"start = numpy.random.randint(0, len(x_data) - 1)\npattern = x_data[start]\nprint(\"Random Seed:\")\nprint(\"\\\"\", ''.join([num_to_char[value] for value in pattern]), \"\\\"\")","metadata":{"execution":{"iopub.status.busy":"2021-06-19T18:59:07.888770Z","iopub.execute_input":"2021-06-19T18:59:07.889132Z","iopub.status.idle":"2021-06-19T18:59:07.894851Z","shell.execute_reply.started":"2021-06-19T18:59:07.889095Z","shell.execute_reply":"2021-06-19T18:59:07.893888Z"},"trusted":true},"execution_count":80,"outputs":[{"name":"stdout","text":"Random Seed:\n\" eral relations waldman fellow professor would lecture upon chemistry alternate days omitted returned \"\n","output_type":"stream"}]},{"cell_type":"code","source":"for i in range(100):\n    x = numpy.reshape(pattern, (1, len(pattern), 1))\n    x = x / float(vocab_len)\n    prediction = model.predict(x, verbose=0)\n    index = numpy.argmax(prediction)\n    result = num_to_char[index]\n\n    print(result, sep=' ', end='', flush=True)\n\n    pattern.append(index)\n    pattern = pattern[1:len(pattern)]","metadata":{"execution":{"iopub.status.busy":"2021-06-19T18:59:22.760440Z","iopub.execute_input":"2021-06-19T18:59:22.760790Z","iopub.status.idle":"2021-06-19T18:59:27.141904Z","shell.execute_reply.started":"2021-06-19T18:59:22.760760Z","shell.execute_reply":"2021-06-19T18:59:27.141067Z"},"trusted":true},"execution_count":82,"outputs":[{"name":"stdout","text":"ected secret also houre moritz along streets work dear consolation sense tears shall see soon contem","output_type":"stream"}]},{"cell_type":"code","source":"import shutil\nshutil.make_archive('model_weights_saved_e300.hdf5', 'zip', '../output/kaggle/working/')","metadata":{"execution":{"iopub.status.busy":"2021-06-19T19:06:46.637054Z","iopub.execute_input":"2021-06-19T19:06:46.637411Z","iopub.status.idle":"2021-06-19T19:06:48.585872Z","shell.execute_reply.started":"2021-06-19T19:06:46.637377Z","shell.execute_reply":"2021-06-19T19:06:48.584859Z"},"trusted":true},"execution_count":87,"outputs":[{"execution_count":87,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/model_weights_saved_e300.hdf5.zip'"},"metadata":{}}]},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'../output/kaggle/working/model_weights_saved_e200.hdf5')\n","metadata":{"execution":{"iopub.status.busy":"2021-06-19T19:02:27.249990Z","iopub.execute_input":"2021-06-19T19:02:27.250356Z","iopub.status.idle":"2021-06-19T19:02:27.257226Z","shell.execute_reply.started":"2021-06-19T19:02:27.250323Z","shell.execute_reply":"2021-06-19T19:02:27.255903Z"},"trusted":true},"execution_count":84,"outputs":[{"execution_count":84,"output_type":"execute_result","data":{"text/plain":"/kaggle/output/kaggle/working/model_weights_saved_e200.hdf5","text/html":"<a href='../output/kaggle/working/model_weights_saved_e200.hdf5' target='_blank'>../output/kaggle/working/model_weights_saved_e200.hdf5</a><br>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}